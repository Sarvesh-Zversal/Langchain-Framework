{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2596e1e5",
   "metadata": {},
   "source": [
    "## LLm Powerd ChatBot\n",
    "#### This Chatbot will able to have conversation and remember previous intraction\n",
    "\n",
    "\n",
    "\n",
    "###### Note that this chatbot we build will only use the language model to have a conversation. There are several other related concept that you may be looking for \n",
    "\n",
    "###### - Conversational RAG : Enable a chatbot experience over an external source of data\n",
    "\n",
    "###### - Agent : Build a chatbot that can take action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426c6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Open AI Key and open source model configuration\n",
    "import os \n",
    "from  dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## for langsmith tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT_NAME\"] = \"LangchainFramework\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8446fe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarvesh\\Desktop\\LangchainFramework\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "c:\\Users\\sarvesh\\Desktop\\LangchainFramework\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000019838E6C590>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000019838E6D010>, model_name='llama-3.3-70b-versatile', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "ChatGroq.api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a8ecac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"Nice to meet you, Sarvesh! As a data scientist, you must be working with data to extract insights and knowledge. That's a fascinating field, and I'm sure you have a lot of exciting projects and experiences to share.\\n\\nWhat kind of data science work do you do, Sarvesh? Are you working in a specific industry, such as finance, healthcare, or technology? Or are you working on more general projects, like machine learning or natural language processing? I'm curious to know more about your work!\", generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content=\"Nice to meet you, Sarvesh! As a data scientist, you must be working with data to extract insights and knowledge. That's a fascinating field, and I'm sure you have a lot of exciting projects and experiences to share.\\n\\nWhat kind of data science work do you do, Sarvesh? Are you working in a specific industry, such as finance, healthcare, or technology? Or are you working on more general projects, like machine learning or natural language processing? I'm curious to know more about your work!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 107, 'prompt_tokens': 47, 'total_tokens': 154, 'completion_time': 0.290598461, 'completion_tokens_details': None, 'prompt_time': 0.004179595, 'prompt_tokens_details': None, 'queue_time': 0.054496436, 'total_time': 0.294778056}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_68f543a7cc', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019be969-e56b-7160-bec7-6efc8956ca67-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 47, 'output_tokens': 107, 'total_tokens': 154}))]], llm_output={'token_usage': {'completion_tokens': 107, 'prompt_tokens': 47, 'total_tokens': 154, 'completion_time': 0.290598461, 'completion_tokens_details': None, 'prompt_time': 0.004179595, 'prompt_tokens_details': None, 'queue_time': 0.054496436, 'total_time': 0.294778056}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_68f543a7cc', 'service_tier': 'on_demand'}, run=[RunInfo(run_id=UUID('019be969-e56b-7160-bec7-6efc8956ca67'))], type='LLMResult')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage    \n",
    "response = model.generate([ [HumanMessage(content=\"My name is sarvesh and I am a data scientist\")]])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9fb784c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is Sarvesh, and you're a data scientist.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 124, 'total_tokens': 139, 'completion_time': 0.029533715, 'completion_tokens_details': None, 'prompt_time': 0.022913776, 'prompt_tokens_details': None, 'queue_time': 0.058657244, 'total_time': 0.052447491}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_c06d5113ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019be969-e869-70e1-9d9e-d8ec49deba7f-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 124, 'output_tokens': 15, 'total_tokens': 139})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "messages = [\n",
    "    HumanMessage(content=\"My name is sarvesh and I am a data scientist\"),\n",
    "    AIMessage(content=\"Nice to meet you, Sarvesh. As a data scientist, you must be working with complex data sets, developing predictive models, and uncovering insights to drive business decisions. What specific areas of data science interest you the most? Are you working on any exciting projects currently?\"),\n",
    "    HumanMessage(content=\"hey what is my name and what do i do?\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95919f9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33564014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9a69399",
   "metadata": {},
   "source": [
    "### Message history\n",
    "Message history in a chatbot is a stored record of the entire conversation (user inputs and AI responses) that provides context and continuity, allowing the AI to understand previous turns and generate relevant, coherent replies, rather than treating each message as a new, isolated request, often managed as a list of messages with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acae4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import  ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory \n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]    \n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68023c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea6f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "responce = with_message_history.invoke( \n",
    "    [HumanMessage(content=\"My name is sarvesh and I am a data scientist\")],\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1b8184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Sarvesh! As a data scientist, you must be working with data to extract insights and knowledge. That's a fascinating field, and I'm sure you have a lot of exciting projects and experiences to share.\\n\\nWhat kind of data science work do you do, Sarvesh? Are you working in a specific industry, such as finance, healthcare, or technology? Or are you working on more general projects, like machine learning or natural language processing? I'm curious to know more about your work!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responce.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5e2a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is Sarvesh, and you're a data scientist.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responce = with_message_history.invoke( \n",
    "    [HumanMessage(content=\"hey what is the my name \")],\n",
    "    config=config\n",
    ")\n",
    "responce.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21765dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about your name. This is the beginning of our conversation, and I don't have any prior knowledge about you. If you'd like to share your name, I'd be happy to chat with you and get to know you better! What's your name?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### change the config --- session id\n",
    "config1 = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "\n",
    "responce = with_message_history.invoke( \n",
    "    [HumanMessage(content=\"hey what is the my name \")],\n",
    "    config=config1\n",
    ")\n",
    "responce.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc374f",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "LangChain uses Prompt Templates to create dynamic and reusable prompts by incorporating variables and formatting options. They are essential for structuring interactions with language models (LLMs) and chat models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2bfe55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prompt template \n",
    "from langchain_core.prompts import  ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "[\n",
    "    ( \"system\",\"You are a helpful assitant. answer all the question to the nest of your abilit\"),\n",
    "    MessagesPlaceholder(variable_name=\"message\")\n",
    "]\n",
    ")\n",
    "\n",
    "chain = prompt|model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20e12a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Sarvesh! It's nice to meet you. Is there something I can help you with or would you like to chat? I'm here to assist you with any questions or topics you'd like to discuss. How's your day going so far?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 61, 'total_tokens': 114, 'completion_time': 0.096827284, 'completion_tokens_details': None, 'prompt_time': 0.002173601, 'prompt_tokens_details': None, 'queue_time': 0.053324789, 'total_time': 0.099000885}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_68f543a7cc', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019be969-ef95-7551-806f-c42e431e7865-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 61, 'output_tokens': 53, 'total_tokens': 114})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"message\": [HumanMessage(content=\"Hii my name is sarvesh\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bc1b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ae356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a9ed181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Sarvesh! It's nice to meet you. Is there something I can help you with or would you like to chat? I'm here to assist you with any questions or topics you'd like to discuss. How's your day going so far?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### change the config --- session id\n",
    "\n",
    "config = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "\n",
    "responce = with_message_history.invoke( \n",
    "    [HumanMessage(content=\"hey  my name is sarvesh\")],\n",
    "    config=config\n",
    ")\n",
    "responce.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1ef8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prompt template with add more complexity\n",
    "from langchain_core.prompts import  ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "[\n",
    "    ( \"system\",\"You are a helpful assitant. answer all the question to the best of your abilit in {language}\"),\n",
    "    MessagesPlaceholder(variable_name=\"message\")\n",
    "]\n",
    ")\n",
    "\n",
    "chain = prompt|model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c40792",
   "metadata": {},
   "outputs": [],
   "source": [
    "responce = chain.invoke({\"message\": [HumanMessage(content=\"Hii my name is sarvesh\")],\"language\":\"Hindi\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d3722f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='नमस्ते सर्वेश! मैं आपका सहायक हूँ। मैं आपकी किस प्रकार से मदद कर सकता हूँ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 63, 'total_tokens': 100, 'completion_time': 0.103473387, 'completion_tokens_details': None, 'prompt_time': 0.011831549, 'prompt_tokens_details': None, 'queue_time': 0.056360021, 'total_time': 0.115304936}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_68f543a7cc', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019be969-f2a9-7a43-8ea4-8423a98b8610-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 63, 'output_tokens': 37, 'total_tokens': 100})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17272a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7982dcbd",
   "metadata": {},
   "source": [
    "######  lets how to wrap this more complicated chain in a message history class . this time , because there are multiple keys in the input , we need to specify to the correct key to use to save the chat history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrap with message history\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"message\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0a720ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते सर्वेश! मैं आपकी मदद करने के लिए तैयार हूँ। आपका दिन कैसा गुजर रहा है? क्या आपके पास कोई प्रश्न या समस्या है जिसमें मैं आपकी सहायता कर सकता हूँ?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### change the config --- session id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"message\": HumanMessage(content=\"hey my name is sarvesh\"),\n",
    "        \"language\": \"Hindi\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ec457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f905fcc2",
   "metadata": {},
   "source": [
    "##### Managing the Conversation History\n",
    "\n",
    "One important concept to understand when building chatots is how to manage conversation history . If left unmanaged , the list of messages will grow unbounded and potentially overflow the context window of the llm , Therefore , it is important to add a step that limit the size of the message you are passing \n",
    "\n",
    "trim_messages = helper to reduce how many messages we are sending to the model the trimer allows us to specify how many tokens we want to keep , along with other parameter like if we want to always keep the system message and weather to allow partial message \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91373c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi! How can I help you?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Explain LangChain in simple terms', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=30,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hello\"),\n",
    "    AIMessage(content=\"Hi! How can I help you?\"),\n",
    "    HumanMessage(content=\"Explain LangChain in simple terms\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bff2b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a framework that helps build applications using large language models (LLMs) like AI chatbots. Here\\'s a simplified explanation:\\n\\n**What is LangChain?**\\nLangChain is an open-source framework that allows developers to create applications that interact with large language models. It provides a set of tools and libraries to make it easier to build, deploy, and manage LLM-based applications.\\n\\n**Key Components:**\\n\\n1. **Language Models**: These are the AI models that understand and generate human-like language. Examples include LLaMA, BERT, and transformer-based models.\\n2. **Agents**: These are the components that interact with the language models. Agents can be thought of as \"wrappers\" around the language models, allowing them to perform specific tasks.\\n3. **Chains**: A chain is a sequence of agents that work together to accomplish a task. Each agent in the chain processes the input and passes the output to the next agent, creating a pipeline of processing steps.\\n\\n**How LangChain Works:**\\n\\n1. A user inputs a prompt or question into the application.\\n2. The input is passed to the first agent in the chain, which processes the input using a language model.\\n3. The output from the first agent is passed to the next agent in the chain, which further processes the output.\\n4. This process continues until the final agent in the chain produces the desired output.\\n5. The output is then returned to the user.\\n\\n**Benefits:**\\nLangChain provides several benefits, including:\\n\\n* **Modularity**: Developers can create and reuse agents to build different applications.\\n* **Flexibility**: LangChain supports multiple language models and allows developers to easily switch between them.\\n* **Scalability**: LangChain enables developers to build complex applications by combining multiple agents and language models.\\n\\nIn summary, LangChain is a framework that simplifies the process of building applications using large language models. It provides a modular and flexible way to create applications that can understand and generate human-like language.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using chains to pass one by one \n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(message=itemgetter(\"message\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "responce = chain.invoke(\n",
    "    { \n",
    "    \"message\":message  + [HumanMessage(content=\"Explain LangChain in simple terms\")],\n",
    "    \"language\": \"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "responce.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "909ddda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangChain is an open-source framework for building applications that utilize large language models (LLMs). The components of LangChain can be broken down into several key parts:\\n\\n1. **LLM (Large Language Model)**: This is the core component of LangChain, which is a pre-trained language model that can understand and generate human-like text. Examples of LLMs include transformer-based models like BERT, RoBERTa, and XLNet.\\n2. **Agent**: The agent is the interface between the user and the LLM. It receives input from the user, processes it, and then sends the input to the LLM for processing. The agent can also receive output from the LLM and post-process it before returning it to the user.\\n3. **Index**: The index is a data structure that stores information about the LLM's knowledge and capabilities. It allows the agent to quickly look up information about the LLM's strengths and weaknesses, and to select the most suitable LLM for a given task.\\n4. **Chain**: The chain is a sequence of LLMs that are composed together to perform a complex task. Each LLM in the chain processes the input and output of the previous LLM, allowing the chain to perform tasks that are beyond the capabilities of a single LLM.\\n5. **Tools**: LangChain provides a set of tools that can be used to build and customize applications. These tools include APIs, SDKs, and command-line interfaces that allow developers to interact with the LLM and the agent.\\n6. **Memory**: LangChain includes a memory component that allows the agent to store and retrieve information about the user's interactions with the LLM. This can be used to implement features like conversational memory and personalization.\\n7. **Embeddings**: LangChain uses embeddings to represent text data in a dense vector space. This allows the LLM to perform tasks like semantic search and text classification.\\n8. **Modules**: LangChain includes a set of pre-built modules that can be used to perform specific tasks, such as text classification, sentiment analysis, and question answering. These modules can be customized and extended to meet the needs of specific applications.\\n\\nSome of the key features of LangChain include:\\n\\n* **Modular architecture**: LangChain's modular architecture allows developers to build and customize applications by combining different components and modules.\\n* **Flexible deployment**: LangChain can be deployed in a variety of environments, including cloud, on-premises, and edge devices.\\n* **Scalability**: LangChain is designed to scale horizontally, allowing it to handle large volumes of traffic and data.\\n* **Extensibility**: LangChain provides a set of APIs and tools that allow developers to extend and customize the framework to meet the needs of specific applications.\\n\\nOverall, LangChain provides a flexible and extensible framework for building applications that utilize large language models. Its components and features can be combined and customized to meet the needs of a wide range of applications, from chatbots and virtual assistants to content generation and text analysis.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lets wrap this in the message history\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"message\"\n",
    "    )\n",
    "\n",
    "## change the session id \n",
    "config = {\"configurable\": {\"session_id\": \"chat5\"}}\n",
    "\n",
    "\n",
    "## invoke the model\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"message\": HumanMessage(content=\"what are the component of langchain\"),\n",
    "        \"language\": \"English\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c13fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254d61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42241f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ac0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13da462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d08ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637dcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
