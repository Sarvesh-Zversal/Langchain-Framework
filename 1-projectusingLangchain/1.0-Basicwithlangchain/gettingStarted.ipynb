{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1854e1b0",
   "metadata": {},
   "source": [
    "## Lets start with langchain and  OpenAI \n",
    "\n",
    "#### In the quickstart we will se how to:\n",
    "\n",
    "###### - Get setup with langChain , LangSmith And  LangServe \n",
    "\n",
    "###### - use the most basic and common components of LangChain: propmpt templaetes model and output parsers.\n",
    "\n",
    "###### Build a simple application with Langchain\n",
    "\n",
    "###### - Trace your application with LangSmith \n",
    "\n",
    "###### - Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from  dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## for langsmith tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT_NAME\"] = \"LangchainFramework\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da99d3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-groq\n",
      "  Downloading langchain_groq-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n",
      "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langchain-groq) (1.2.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1.0.0,>=0.30.0->langchain-groq) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.6.3)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sarvesh\\desktop\\langchainframework\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-groq) (2.6.3)\n",
      "Downloading langchain_groq-1.1.1-py3-none-any.whl (19 kB)\n",
      "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
      "Installing collected packages: groq, langchain-groq\n",
      "\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   ---------------------------------------- 0/2 [groq]\n",
      "   -------------------- ------------------- 1/2 [langchain-groq]\n",
      "   -------------------- ------------------- 1/2 [langchain-groq]\n",
      "   -------------------- ------------------- 1/2 [langchain-groq]\n",
      "   ---------------------------------------- 2/2 [langchain-groq]\n",
      "\n",
      "Successfully installed groq-0.37.1 langchain-groq-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f79eda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and configure the Groq LLM\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\", \n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644d6d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a type of artificial intelligence that is capable of generating new, original content, such as images, videos, music, text, or even entire datasets. This is in contrast to traditional AI, which is typically focused on analyzing and processing existing data.\n",
      "\n",
      "Generative AI models use complex algorithms and neural networks to learn patterns and relationships within a dataset, and then use this knowledge to create new, synthetic data that is similar in style and structure to the original data. These models can be trained on a wide range of data types, including images, videos, audio, and text.\n",
      "\n",
      "Some common applications of generative AI include:\n",
      "\n",
      "1. **Image and video generation**: Generative AI can be used to create realistic images and videos of objects, scenes, and people. This has applications in fields such as computer vision, robotics, and entertainment.\n",
      "2. **Text generation**: Generative AI can be used to generate human-like text, such as articles, stories, and dialogue. This has applications in fields such as content creation, language translation, and chatbots.\n",
      "3. **Music and audio generation**: Generative AI can be used to create original music and audio tracks, such as songs, sound effects, and voiceovers.\n",
      "4. **Data augmentation**: Generative AI can be used to generate new data that is similar to existing data, which can be used to augment datasets and improve the performance of machine learning models.\n",
      "5. **Art and design**: Generative AI can be used to create original artwork, such as paintings, sculptures, and designs.\n",
      "\n",
      "Some of the key techniques used in generative AI include:\n",
      "\n",
      "1. **Generative Adversarial Networks (GANs)**: GANs consist of two neural networks that work together to generate new data. One network generates new data, while the other network evaluates the generated data and provides feedback to the first network.\n",
      "2. **Variational Autoencoders (VAEs)**: VAEs are a type of neural network that learns to compress and reconstruct data. They can be used to generate new data by sampling from the compressed representation.\n",
      "3. **Recurrent Neural Networks (RNNs)**: RNNs are a type of neural network that is well-suited to generating sequential data, such as text or music.\n",
      "\n",
      "Overall, generative AI has the potential to revolutionize a wide range of fields, from entertainment and art to science and engineering. However, it also raises important questions about the ethics and implications of creating and using synthetic data.\n"
     ]
    }
   ],
   "source": [
    "# Input and get responce from LLM\n",
    "\n",
    "response = llm.invoke(\"What is generative AI?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63625a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a an expert in AI engineer. provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we see different chatprompt template with langchain \n",
    "## ChatPromptTemplate allows us to create more complex prompts with multiple messages and roles.\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a an expert in AI engineer. provide me answer based on the question\"),\n",
    "        \n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a33e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a an expert in AI engineer. provide me answer based on the question', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Create a Node.js project to send emails using AWS SES SDK v3.\\n\\nRequirements:\\n1. Use '@aws-sdk/client-ses'.\\n2. Create a 'sesClient.js' to configure the SES client (Region: us-east-1).\\n3. Create an 'index.js' with a function 'sendEmail(toAddress, subject, body)'.\\n4. Include a 'package.json' with the AWS SDK dependency.\\n5. Use placeholder credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY) in a .env file structure.\\n6. Add a README with instructions on how to verify email identities in the AWS Console.\\n\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "## creating the chain with prompt template\n",
    "\n",
    "## Chain is basically mean a sequence of actions that are performed in order to achieve a specific task.\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"question\": \"Explain deep copy and shallow copy in Python?\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4044e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a an expert in AI engineer. provide me answer based on the question', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Create a Node.js project to send emails using AWS SES SDK v3.\\n\\nRequirements:\\n1. Use '@aws-sdk/client-ses'.\\n2. Create a 'sesClient.js' to configure the SES client (Region: us-east-1).\\n3. Create an 'index.js' with a function 'sendEmail(toAddress, subject, body)'.\\n4. Include a 'package.json' with the AWS SDK dependency.\\n5. Use placeholder credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY) in a .env file structure.\\n6. Add a README with instructions on how to verify email identities in the AWS Console.\\n\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "### Now we use different output parsers\n",
    "#output parser mean we can parse the output of the LLM into a specific format like JSON, list, etc.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "response = chain.invoke({\"question\": \"List down the programming languages used in Data Science\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6ba717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "\n",
    "class SuperiorDeveloperAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"Initializing SuperiorDeveloperAgent (Groq)...\")\n",
    "\n",
    "        load_dotenv()\n",
    "        os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        self.parser = StrOutputParser()\n",
    "        self.search = DuckDuckGoSearchRun()\n",
    "\n",
    "    def parse_and_create(self, text, base_dir=\"converted_project\"):\n",
    "        if \"FILE:\" not in text:\n",
    "            raise ValueError(\" No FILE blocks found in model output\")\n",
    "\n",
    "        if os.path.exists(base_dir):\n",
    "            shutil.rmtree(base_dir)\n",
    "\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "        files = re.findall(\n",
    "            r'FILE:\\s*([\\w\\.\\-/]+)\\n(.*?)(?=FILE:|$)',\n",
    "            text,\n",
    "            re.DOTALL\n",
    "        )\n",
    "\n",
    "        for filename, content in files:\n",
    "            full_path = os.path.join(base_dir, filename.strip())\n",
    "            os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "\n",
    "            with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(content.strip())\n",
    "\n",
    "            print(f\" Created: {full_path}\")\n",
    "\n",
    "    def run_conversion(self, prompt_text, target_lang=\"Node.js\"):\n",
    "        full_prompt = f\"\"\"\n",
    "You are a Senior Software Engineer.\n",
    "\n",
    "Create a COMPLETE {target_lang} project.\n",
    "\n",
    "STRICT RULES:\n",
    "- Output ONLY file contents\n",
    "- EVERY file MUST start with: FILE: path/to/file\n",
    "- NO markdown\n",
    "- NO explanation\n",
    "\n",
    "User request:\n",
    "{prompt_text}\n",
    "\"\"\"\n",
    "\n",
    "        print(\"Generating project using Groq...\")\n",
    "        response = self.llm.invoke(full_prompt)\n",
    "\n",
    "        self.parse_and_create(response.content)\n",
    "        self.run_npm_install()\n",
    "\n",
    "    def run_npm_install(self, base_dir=\"converted_project\"):\n",
    "        package_json = os.path.join(base_dir, \"package.json\")\n",
    "        if not os.path.exists(package_json):\n",
    "            print(\" package.json not found, skipping npm install\")\n",
    "            return\n",
    "\n",
    "        print(\"Installing npm dependencies...\")\n",
    "\n",
    "        subprocess.run(\n",
    "            \"npm install\",\n",
    "            cwd=base_dir,\n",
    "            shell=True   # âœ… Windows-safe\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ab9b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SuperiorDeveloperAgent (Groq)...\n",
      "Generating project using Groq...\n",
      " Created: converted_project\\package.json\n",
      " Created: converted_project\\.env.example\n",
      " Created: converted_project\\sesClient.js\n",
      " Created: converted_project\\index.js\n",
      " Created: converted_project\\README.md\n",
      "Installing npm dependencies...\n"
     ]
    }
   ],
   "source": [
    "agent = SuperiorDeveloperAgent()\n",
    "\n",
    "\n",
    "agent.run_conversion(\"\"\"\n",
    "Create a COMPLETE Node.js project to send emails using AWS SES SDK v3.\n",
    "\n",
    "STRICT RULES (DO NOT BREAK):\n",
    "- Output ONLY file contents\n",
    "- EVERY file MUST start with: FILE: relative/path/filename\n",
    "- NO explanations, NO markdown, NO extra text\n",
    "\n",
    "Requirements:\n",
    "1. Use '@aws-sdk/client-ses'.\n",
    "2. Create 'sesClient.js' to configure SES client (Region: us-east-1).\n",
    "3. Create 'index.js' with a function:\n",
    "   sendEmail(toAddress, subject, body)\n",
    "4. Include a 'package.json' with required dependencies.\n",
    "5. Include a '.env.example' file with:\n",
    "   AWS_ACCESS_KEY_ID=\n",
    "   AWS_SECRET_ACCESS_KEY=\n",
    "6. Add a README.md explaining:\n",
    "   - How to verify email identities in AWS SES Console\n",
    "   - How to run the project locally\n",
    "\"\"\", target_lang=\"Node.js\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106fd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
